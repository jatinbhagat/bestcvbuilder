#!/usr/bin/env python3
"""
Enhanced TXT Report Generator with all 25 categories and Gemini-powered fix suggestions
"""

import os
import sys
import re
from typing import List, Dict, Any, Optional
import logging

# Add the project root to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Add cv-parser to path since it uses hyphens
sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'cv-parser'))

# Import existing modules
import importlib.util

try:
    # Add cv-optimizer to path to handle relative imports
    cv_optimizer_path = os.path.join(os.path.dirname(__file__), 'cv-optimizer')
    if cv_optimizer_path not in sys.path:
        sys.path.insert(0, cv_optimizer_path)
    
    # Import prompt templates first
    import prompt_templates
    
    # Import gemini_client
    import gemini_client
    GeminiOptimizer = gemini_client.GeminiOptimizer
    GEMINI_CLIENT_AVAILABLE = True
    print("‚úÖ Successfully imported Gemini client")
except Exception as e:
    print(f"Warning: Could not import Gemini client: {e}")
    GEMINI_CLIENT_AVAILABLE = False

try:
    # Import directly from cv-parser/index.py
    spec = importlib.util.spec_from_file_location("cv_parser_index", 
                                                  os.path.join(os.path.dirname(__file__), 'cv-parser', 'index.py'))
    cv_parser_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(cv_parser_module)
    generate_comprehensive_ats_scores_frontend = cv_parser_module.generate_comprehensive_ats_scores_frontend
    
    # Import specific analysis functions for detailed breakdown
    analyze_verb_tenses_frontend = cv_parser_module.analyze_verb_tenses_frontend
    analyze_repetition_frontend = cv_parser_module.analyze_repetition_frontend
    analyze_personal_pronouns_frontend = cv_parser_module.analyze_personal_pronouns_frontend
    analyze_date_formatting = cv_parser_module.analyze_date_formatting
    get_enhanced_issue_description = cv_parser_module.get_enhanced_issue_description
    
    CV_PARSER_AVAILABLE = True
except Exception as e:
    print(f"Warning: Could not import CV parser: {e}")
    CV_PARSER_AVAILABLE = False

GEMINI_AVAILABLE = GEMINI_CLIENT_AVAILABLE and CV_PARSER_AVAILABLE

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# NO SAMPLE DATA - All analysis must use real CV input
# This ensures no hardcoded scores or fallback evidence

def get_backend_evidence_and_analysis(category_name: str, resume_text: str, score: int, category_data: dict = None, gemini_client = None) -> Dict[str, str]:
    """Get evidence and detailed analysis from actual backend category data and functions"""
    
    if not CV_PARSER_AVAILABLE:
        logger.error(f"‚ùå Backend unavailable for {category_name} - cannot generate analysis without real data")
        raise ValueError(f"Backend CV parser required for {category_name} analysis - no fallbacks allowed")
    
    try:
        import re
        from collections import Counter
        
        category_lower = category_name.lower().replace(' ', '_')
        
        # Get enhanced issue description from backend for all categories
        try:
            enhanced_desc = get_enhanced_issue_description(category_name, score, resume_text)
            
            # Extract evidence from enhanced description
            evidence = "Backend analysis completed"
            if enhanced_desc and 'issue' in enhanced_desc and enhanced_desc['issue'] and enhanced_desc['issue'].strip():
                evidence = enhanced_desc['issue'][:100] + "..." if len(enhanced_desc['issue']) > 100 else enhanced_desc['issue']
            elif score < 8:
                evidence = "Issues detected in backend analysis"
            else:
                evidence = "None flagged"
            
            analysis_text = f'Backend analysis: {category_name} scored {score}/10'
            if enhanced_desc and 'understanding' in enhanced_desc and enhanced_desc['understanding'] and enhanced_desc['understanding'].strip():
                analysis_text = enhanced_desc['understanding']
            
            penalties = f'Score: {score}/10 points'
            if score < 10:
                penalties = f'Deductions applied: -{10-score} points (10 ‚Üí {score})'
            
            rule_explanation = f'ATS scoring rules applied to {category_name}'
            
            logger.debug(f"‚úÖ Enhanced backend analysis for {category_name}: Evidence='{evidence[:50]}...'")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not get enhanced description for {category_name}: {e}")
            # Fall back to manual analysis for key categories
            enhanced_desc = None
            evidence = "Backend analysis completed"
            analysis_text = f'Backend analysis: {category_name} scored {score}/10'
            penalties = f'Score: {score}/10 points'
            rule_explanation = f'ATS scoring rules applied to {category_name}'
        
        # Special detailed analysis for key categories that need specific evidence extraction
        if category_lower == 'grammar':
            # Use Gemini LLM for Grammar analysis
            if gemini_client and hasattr(gemini_client, 'model') and gemini_client.model:
                logger.info(f"üß† Using Gemini LLM for Grammar analysis")
                try:
                    grammar_prompt = f"""Analyze this resume text for grammar errors. Find specific examples of grammar mistakes.
                    
Resume Text: {resume_text[:1000]}

Instructions:
- Identify specific grammar errors with exact quotes
- Focus on common resume mistakes: subject-verb disagreement, tense errors, comma usage
- Return only the first 2-3 specific examples found
- Format: "Error found: [exact quote from text]"

If no errors found, respond with "No grammar errors detected\""""
                    
                    response_text, _ = gemini_client._make_gemini_request(grammar_prompt, max_tokens=200)
                    
                    if "No grammar errors detected" not in response_text:
                        evidence = response_text.strip()[:100] + "..." if len(response_text) > 100 else response_text.strip()
                    else:
                        evidence = "None flagged"
                        
                    analysis = enhanced_desc.get('understanding', 'Grammar evaluation completed using AI analysis') if enhanced_desc else 'Grammar evaluation completed using AI analysis'
                    
                    return {
                        'evidence': evidence,
                        'analysis': analysis,
                        'penalties': penalties,
                        'rule_explanation': 'Grammar errors can cause ATS rejection and suggest lack of attention to detail'
                    }
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Gemini grammar analysis failed: {e}")
                    # Fall through to standard analysis
            else:
                logger.error(f"‚ö†Ô∏è Gemini client not available for Grammar analysis - Grammar MUST use Gemini exclusively")
                raise ValueError(f"Grammar analysis requires Gemini LLM but client is not available")
                    
        elif category_lower == 'spelling':
            # Use Gemini LLM for Spelling analysis
            if gemini_client and hasattr(gemini_client, 'model') and gemini_client.model:
                logger.info(f"üß† Using Gemini LLM for Spelling analysis")
                try:
                    spelling_prompt = f"""Check this resume text for spelling errors. Find specific spelling mistakes.
                    
Resume Text: {resume_text[:1000]}

Instructions:
- Identify specific spelling errors with exact quotes from the text
- Look for common resume spelling mistakes: typos, wrong word usage, technical terms
- Return only the first 2-3 specific examples found  
- Format: "Spelling error: [exact misspelled word/phrase from text]"

If no errors found, respond with "No spelling errors detected\""""
                    
                    response_text, _ = gemini_client._make_gemini_request(spelling_prompt, max_tokens=200)
                    
                    if "No spelling errors detected" not in response_text:
                        evidence = response_text.strip()[:100] + "..." if len(response_text) > 100 else response_text.strip()
                    else:
                        evidence = "None flagged"
                        
                    analysis = enhanced_desc.get('understanding', 'Spelling evaluation completed using AI analysis') if enhanced_desc else 'Spelling evaluation completed using AI analysis'
                    
                    return {
                        'evidence': evidence,
                        'analysis': analysis,
                        'penalties': penalties,
                        'rule_explanation': 'Spelling errors can cause automatic ATS rejection and harm credibility'
                    }
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Gemini spelling analysis failed: {e}")
                    raise ValueError(f"Spelling analysis failed: {e}")
            else:
                logger.error(f"‚ö†Ô∏è Gemini client not available for Spelling analysis - Spelling MUST use Gemini exclusively")
                raise ValueError(f"Spelling analysis requires Gemini LLM but client is not available")
                    
        elif category_lower == 'verb_tenses':
            # Replicate backend verb tense analysis
            past_tense_verbs = ['developed', 'created', 'managed', 'led', 'implemented', 'designed', 'achieved', 'delivered']
            present_tense_verbs = ['develop', 'create', 'manage', 'lead', 'implement', 'design', 'achieve', 'deliver']
            
            text_lower = resume_text.lower()
            past_count = sum(1 for verb in past_tense_verbs if verb in text_lower)
            present_count = sum(1 for verb in present_tense_verbs if verb in text_lower)
            
            # Find first evidence
            evidence = "No verbs found"
            for verb in past_tense_verbs + present_tense_verbs:
                match = re.search(rf'\b{verb}\b.*', resume_text, re.IGNORECASE)
                if match:
                    evidence = match.group(0)[:60] + "..." 
                    break
            
            if present_count > past_count:
                analysis = f'Poor tense usage: Found {present_count} present tense vs {past_count} past tense verbs'
                penalties = f'Too many present tense verbs: -6 points (10 ‚Üí 4)'
            else:
                analysis = f'Mixed tenses: Found {past_count} past tense and {present_count} present tense verbs'
                penalties = f'Tense inconsistency penalty: -{10-score} points (10 ‚Üí {score})'
                
            return {
                'evidence': evidence,
                'analysis': analysis,
                'penalties': penalties,
                'rule_explanation': 'Use past tense for previous roles, present tense only for current position'
            }
            
        elif category_lower == 'personal_pronouns':
            # Replicate backend personal pronoun analysis  
            pronoun_patterns = [r'\bi\b', r'\bme\b', r'\bmy\b', r'\bmyself\b', r'\bour\b', r'\bwe\b']
            
            found_pronouns = []
            evidence = "None flagged"
            for pattern in pronoun_patterns:
                matches = re.findall(pattern, resume_text, re.IGNORECASE)
                if matches:
                    found_pronouns.extend(matches)
                    # Find first occurrence with context
                    match = re.search(rf'.{{0,30}}{pattern}.{{0,30}}', resume_text, re.IGNORECASE)
                    if match:
                        evidence = match.group(0).strip()
                        break
            
            if len(found_pronouns) == 0:
                analysis = 'Excellent: No personal pronouns found'
                penalties = f'No penalties (score: {score}/10)'
            else:
                analysis = f'Found {len(found_pronouns)} personal pronouns: {", ".join(set(found_pronouns[:3]))}'
                penalty = min(6, len(found_pronouns))
                penalties = f'Personal pronoun penalty: -{penalty} points (10 ‚Üí {10-penalty})'
                
            return {
                'evidence': evidence,
                'analysis': analysis,
                'penalties': penalties,
                'rule_explanation': 'Remove all first-person pronouns (I, me, my, we, our)'
            }
            
        elif category_lower == 'repetition':
            # Replicate backend repetition analysis
            action_verbs_patterns = [
                r'\b(manage[ds]?|managing)\b', r'\b(develop[eds]?|developing)\b', 
                r'\b(creat[ed]?|creating)\b', r'\b(implement[eds]?|implementing)\b',
                r'\b(lead[s]?|leading|led)\b', r'\b(design[eds]?|designing)\b'
            ]
            
            verb_matches = []
            evidence = "None flagged"
            for pattern in action_verbs_patterns:
                matches = re.findall(pattern, resume_text, re.IGNORECASE)
                if matches:
                    verb_matches.extend([match[0] if isinstance(match, tuple) else match for match in matches])
                    if evidence == "None flagged":
                        match = re.search(rf'.{{0,30}}{pattern}.{{0,30}}', resume_text, re.IGNORECASE)
                        if match:
                            evidence = match.group(0).strip()
            
            verb_counts = Counter(verb_matches)
            repeated_verbs = {verb: count for verb, count in verb_counts.items() if count > 1}
            
            if not repeated_verbs:
                analysis = 'Good verb variety: No repeated action verbs found'  
                penalties = f'No repetition penalties (score: {score}/10)'
            else:
                analysis = f'Verb repetition detected: {dict(repeated_verbs)}'
                total_repetition = sum(count - 1 for count in repeated_verbs.values())
                penalty = min(10, total_repetition * 2)
                # Fix: Calculate penalty to match actual score
                actual_penalty = 10 - score
                penalties = f'Repetition penalty: -{actual_penalty} points (10 ‚Üí {score})'
                
            return {
                'evidence': evidence,
                'analysis': analysis,
                'penalties': penalties,
                'rule_explanation': 'Vary action verbs - deduct 2 points per repeated verb occurrence'
            }
            
        elif category_lower == 'dates':
            # Replicate backend date analysis
            date_patterns = [
                (r'\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{4}\b', 'Month YYYY'),
                (r'\b\d{4}\s*[-‚Äì]\s*(present|Present|\d{4})\b', 'YYYY-Present')
            ]
            
            all_dates = []
            format_types = []
            evidence = "None flagged"
            
            for pattern, format_name in date_patterns:
                matches = re.findall(pattern, resume_text, re.IGNORECASE)
                if matches:
                    all_dates.extend(matches)
                    format_types.extend([format_name] * len(matches))
                    if evidence == "None flagged":
                        match = re.search(rf'.{{0,30}}{pattern}.{{0,30}}', resume_text, re.IGNORECASE)
                        if match:
                            evidence = match.group(0).strip()
            
            if len(set(format_types)) <= 1 and len(all_dates) > 0:
                analysis = f'Good date consistency: Found {len(all_dates)} dates in consistent format'
                penalties = f'No date formatting penalties (score: {score}/10)'
            elif len(all_dates) == 0:
                analysis = 'No employment or education dates found in resume'
                penalties = f'Missing dates penalty: -{10-score} points (10 ‚Üí {score})'
            else:
                analysis = f'Date inconsistency: Found {len(set(format_types))} different formats: {", ".join(set(format_types))}'
                penalty = len(set(format_types)) * 2
                penalties = f'Format inconsistency penalty: -{penalty} points'
                
            return {
                'evidence': evidence,
                'analysis': analysis,
                'penalties': penalties,
                'rule_explanation': 'Consistent date formatting required across all work/education entries'
            }
            
        else:
            # For all other categories, use the enhanced backend analysis we already retrieved
            return {
                'evidence': evidence,
                'analysis': analysis_text,
                'penalties': penalties,
                'rule_explanation': rule_explanation
            }
        
    except Exception as e:
        logger.error(f"‚ùå Failed to get backend analysis for {category_name}: {e}")
        raise ValueError(f"Backend analysis failed for {category_name}: {e}")

def get_why_matters_explanation(category_name: str, score: int, category_data: dict = None) -> str:
    """Get 'Why this matters' explanation, preferably from backend data"""
    
    # Try to get from backend category data first
    if category_data and 'why_matters' in category_data:
        return category_data['why_matters']
    
    # Fallback to predefined explanations (but log this as non-ideal)
    explanations = {
        'Contact Details': "ATS systems need properly formatted contact information to reach you for interviews.",
        'Education Section': "Education formatting affects ATS parsing and recruiter confidence in your qualifications.",
        'Skills Section': "Properly organized skills help ATS match you to job requirements and keywords.",
        'Analytical': "Quantified analytical achievements demonstrate measurable business impact to employers.",
        'Leadership': "Leadership examples show career growth potential and management capabilities.",
        'Page Density': "Proper white space and layout improve ATS readability and human reviewer experience.",
        'Use of Bullets': "Consistent bullet formatting helps ATS parse your achievements correctly.",
        'Grammar': "Grammar errors create negative first impressions and suggest lack of attention to detail.",
        'Spelling': "Spelling mistakes can cause automatic ATS rejection and harm professional credibility.",
        'Verb Tenses': "Consistent tenses (past for old roles, present for current) improve professional presentation.",
        'Personal Pronouns': "Removing 'I', 'me', 'my' makes your resume more professional and ATS-friendly.",
        'Quantifiable Achievements': "Numbers and metrics prove your impact and help you stand out to recruiters.",
        'Action Verbs': "Strong action verbs create impact and help ATS categorize your experience correctly.",
        'Active Voice': "Active voice makes your achievements more powerful and easier for ATS to parse.",
        'Summary': "Professional summaries without pronouns improve ATS keyword matching and readability.",
        'Teamwork': "Collaboration examples demonstrate soft skills that employers highly value.",
        'Verbosity': "Concise language improves readability and keeps recruiters engaged.",
        'Repetition': "Varied vocabulary prevents ATS keyword stuffing penalties and shows communication skills.",
        'Unnecessary Sections': "Removing outdated sections focuses attention on relevant qualifications.",
        'Growth Signals': "Career progression evidence shows ambition and capability for advancement.",
        'Drive': "Self-motivation examples demonstrate initiative that employers seek.",
        'Certifications': "Relevant certifications validate skills and improve ATS keyword matching.",
        'Dates': "Consistent date formatting helps ATS parse your work history accurately.",
        'CV Readability Score': "Overall readability affects both ATS parsing success and human reviewer engagement."
    }
    
    explanation = explanations.get(category_name, f"{category_name} optimization improves ATS compatibility and recruiter appeal.")
    
    # Log when using fallback
    if not category_data or 'why_matters' not in category_data:
        logger.debug(f"‚ö†Ô∏è Using fallback explanation for {category_name}")
    
    return explanation

def get_score_label(score: int) -> str:
    """Get user-friendly score label"""
    if score >= 80:  # For overall percentage scores
        return "Pass"
    elif score >= 50:
        return "Needs Attention"
    elif score >= 8:  # For individual category scores (out of 10)
        return "Pass"
    elif score >= 5:
        return "Needs Attention"
    else:
        return "Critical"

def verify_no_hardcoded_data(categories: List[dict], resume_text: str) -> Dict[str, Any]:
    """Verify that all analysis comes from real backend data, not hardcoded fallbacks"""
    verification_report = {
        'is_valid': True,
        'issues': [],
        'backend_data_count': 0,
        'fallback_count': 0,
        'evidence_sources': {},
        'enhanced_backend_count': 0
    }
    
    for category in categories:
        category_name = category['name']
        
        # Check if category has enhanced backend analysis (via get_enhanced_issue_description)
        has_enhanced_data = (
            'issue' in category and category['issue'] and category['issue'].strip() and
            'understanding' in category and category['understanding'] and category['understanding'].strip() and
            len(category['understanding']) > 50  # Enhanced understanding is detailed
        )
        
        # Check for specific evidence of real backend analysis
        analysis_indicators = [
            'Evaluates', 'Measures', 'Assesses', 'Detects',  # Enhanced analysis language
            'Backend analysis', 'ATS scoring rules applied to',  # Backend integration
        ]
        
        has_analysis_indicators = False
        if 'understanding' in category and category['understanding']:
            has_analysis_indicators = any(indicator in category['understanding'] for indicator in analysis_indicators)
        
        if has_enhanced_data:
            verification_report['backend_data_count'] += 1
            verification_report['enhanced_backend_count'] += 1
            verification_report['evidence_sources'][category_name] = 'enhanced_backend_extracted'
        elif has_analysis_indicators or ('issue' in category and category['issue'] and category['issue'].strip()):
            verification_report['backend_data_count'] += 1
            verification_report['evidence_sources'][category_name] = 'backend_extracted'  
        else:
            verification_report['fallback_count'] += 1
            verification_report['issues'].append(f"Category '{category_name}' lacks detailed backend analysis")
            verification_report['evidence_sources'][category_name] = 'generic_fallback'
    
    if verification_report['fallback_count'] > 0:
        verification_report['is_valid'] = False
    
    return verification_report

def generate_comprehensive_enhanced_txt_report(resume_text: str = None) -> str:
    """
    Generate comprehensive TXT report with all 25 categories using REAL backend analysis
    
    Args:
        resume_text: REQUIRED - actual CV content to analyze (no defaults/samples allowed)
        
    Returns:
        Comprehensive analysis report based on real backend scoring
    """
    
    if not resume_text or resume_text.strip() == "":
        raise ValueError("‚ùå Resume text is required - no sample data allowed")
    
    logger.info("üöÄ Starting comprehensive enhanced TXT report generation...")
    
    # Initialize Gemini client if available
    gemini_client = None
    if GEMINI_CLIENT_AVAILABLE:
        try:
            gemini_api_key = os.getenv('GEMINI_API_KEY')
            if gemini_api_key:
                gemini_client = GeminiOptimizer(api_key=gemini_api_key)
                logger.info("‚úÖ Gemini client initialized for fix suggestions")
            else:
                logger.warning("‚ö†Ô∏è GEMINI_API_KEY not found, using fallback suggestions")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not initialize Gemini client: {e}")
    else:
        logger.warning("‚ö†Ô∏è Gemini client library not available")
    
    # Get all 25 categories from backend - REQUIRED, no fallbacks
    if not CV_PARSER_AVAILABLE:
        logger.error("‚ùå CV Parser not available - cannot generate report without real backend data")
        raise ValueError("CV Parser module not available - cannot generate analysis without real backend data")
    
    try:
        logger.info(f"üöÄ Analyzing CV content: {len(resume_text)} characters")
        all_categories = generate_comprehensive_ats_scores_frontend(resume_text)
        logger.info(f"‚úÖ Generated {len(all_categories)} categories from real backend analysis")
        
        # Verify we have real data
        if not all_categories or len(all_categories) == 0:
            raise ValueError("Backend returned empty categories - cannot generate report")
            
        # VERIFICATION: Check for hardcoded/fallback data
        verification = verify_no_hardcoded_data(all_categories, resume_text)
        logger.info(f"üîç Verification - Backend data: {verification['backend_data_count']}, Fallbacks: {verification['fallback_count']}")
        
        if not verification['is_valid']:
            logger.warning(f"‚ö†Ô∏è Verification issues found: {verification['issues']}")
        else:
            logger.info("‚úÖ All categories use real backend analysis - no hardcoded data detected")
            
    except Exception as e:
        logger.error(f"‚ùå Failed to generate categories from backend: {e}")
        raise ValueError(f"Backend analysis failed: {e}")
    
    # Calculate overall score
    total_score = sum(cat['score'] for cat in all_categories)
    max_score = len(all_categories) * 10
    overall_percentage = int((total_score / max_score) * 100)
    
    # Categorize by score
    critical_categories = [cat for cat in all_categories if cat['score'] <= 4]
    needs_attention_categories = [cat for cat in all_categories if 5 <= cat['score'] <= 7]
    pass_categories = [cat for cat in all_categories if cat['score'] >= 8]
    
    # Start building report
    report_lines = [
        "=" * 80,
        "ATS COMPREHENSIVE ANALYSIS REPORT - ALL 25 CATEGORIES",
        "=" * 80,
        f"Current ATS Score: {overall_percentage}/100 ‚Äì {get_score_label(overall_percentage)}",
        f"Report Generated: 2025-08-27 12:00:00",
        "",
        "EXECUTIVE DASHBOARD",
        "-" * 40,
        f"üö® Critical Issues (0-4/10): {len(critical_categories)}",
        f"‚ö° Needs Attention (5-7/10): {len(needs_attention_categories)}",
        f"‚úÖ Pass Categories (8-10/10): {len(pass_categories)}",
        f"üìä Total Categories Analyzed: {len(all_categories)}",
        "",
        "üéØ PRIORITY ACTION PLAN",
        "-" * 40,
        "1. Fix Critical Issues first (biggest impact)",
        "2. Address Needs Attention items (quick wins)",
        "3. Maintain Pass categories (keep strong performance)",
        "",
        ""
    ]
    
    # Generate sections for each category type
    for section_title, categories, emoji in [
        ("üö® CRITICAL ISSUES (IMMEDIATE ATTENTION REQUIRED)", critical_categories, "üö®"),
        ("‚ö° NEEDS ATTENTION (IMPROVEMENT OPPORTUNITIES)", needs_attention_categories, "‚ö°"),
        ("‚úÖ PASS CATEGORIES (STRONG PERFORMANCE)", pass_categories, "‚úÖ")
    ]:
        if categories:
            report_lines.extend([
                section_title,
                "=" * 60,
            ])
            
            for i, category in enumerate(categories, 1):
                category_name = category['name']
                score = category['score']
                score_label = get_score_label(score)
                
                # Get backend evidence and analysis using real category data  
                try:
                    backend_analysis = get_backend_evidence_and_analysis(
                        category_name, resume_text, score, category_data=category, gemini_client=gemini_client
                    )
                    logger.debug(f"‚úÖ Backend analysis for {category_name}: Evidence='{backend_analysis['evidence'][:50]}...'")
                except Exception as e:
                    logger.error(f"‚ùå Failed to get backend analysis for {category_name}: {e}")
                    # Skip this category rather than using fallbacks
                    continue
                
                # Get why it matters from backend data or fallback
                why_matters = get_why_matters_explanation(category_name, score, category_data=category)
                
                # Generate fix suggestion using Gemini
                fix_suggestion = "Follow ATS best practices for improvement."
                if gemini_client and hasattr(gemini_client, 'model') and gemini_client.model:
                    try:
                        fix_suggestion = gemini_client.generate_fix_suggestion_with_gemini(
                            category_name, backend_analysis['evidence'], score, why_matters
                        )
                    except Exception as e:
                        logger.warning(f"Failed to get Gemini suggestion for {category_name}: {e}")
                
                # Customize content based on section type
                if score <= 4:  # Critical Issues
                    problem_text = "Problem: Critical ATS compatibility issue"
                    time_text = f"Time to Fix: 10-15 minutes"
                elif score <= 7:  # Needs Attention  
                    problem_text = "Strength: Good foundation with room for improvement"
                    time_text = f"Time to Fix: 5-10 minutes"
                else:  # Pass Categories
                    problem_text = "Strength: Excellent performance in this area"
                    time_text = f"Minor polish: 2-5 minutes"
                    
                # Add category block with enhanced details
                report_lines.extend([
                    f"{i}. {category_name.upper()}: {category_name} Analysis",
                    f"   Current Score: {score}/10 ‚Äì {score_label} | {time_text}",
                    f"   {problem_text}",
                    "",
                    f"   üí° SCORING BREAKDOWN:",
                    f"   ATS Rule: {backend_analysis['rule_explanation']}",
                    f"   Analysis: {backend_analysis['analysis']}",
                    f"   Penalties Applied: {backend_analysis['penalties']}",
                    "",
                    f"   **Evidence**: {backend_analysis['evidence']}",
                    f"   **Why this matters**: {why_matters}",
                    f"   **Fix**: {fix_suggestion}",
                    "",
                    "-" * 50,
                    ""
                ])
            
            report_lines.append("")
    
    report_lines.extend([
        "üî• IMMEDIATE NEXT STEPS",
        "=" * 60,
        "1. Print this report or keep it open while editing",
        "2. Work through Critical Issues first - they have the biggest impact",  
        "3. Use the Evidence quotes to find exact problems in your resume",
        "4. Apply the Fix suggestions to improve each category",
        "5. Re-upload your resume to see score improvement",
        "",
        "üí° PRO TIPS FOR MAXIMUM ATS SUCCESS",
        "=" * 60,
        "‚Ä¢ Each fix above was AI-generated specifically for your resume",
        "‚Ä¢ Evidence shows the exact text that caused scoring issues",
        "‚Ä¢ Focus on Critical and Needs Attention categories first",
        "‚Ä¢ Even Pass categories can be improved for perfection",
        "‚Ä¢ Save final resume as PDF to preserve formatting",
        "",
        "Generated by BestCVBuilder.com with Gemini AI Enhancement",
        "For more optimization: https://bestcvbuilder.com",
        "=" * 80
    ])
    
    # Add verification footer to report
    verification = verify_no_hardcoded_data(all_categories, resume_text)
    report_lines.extend([
        "",
        "üîç DATA VERIFICATION REPORT",
        "=" * 60,
        f"‚úÖ Categories with real backend analysis: {verification['backend_data_count']}",
        f"üöÄ Categories with enhanced backend analysis: {verification.get('enhanced_backend_count', 0)}",
        f"‚ö†Ô∏è Categories using fallback analysis: {verification['fallback_count']}",
        f"üìä CV content analyzed: {len(resume_text)} characters",
        f"üéØ Analysis validity: {'VERIFIED - All real backend data' if verification['is_valid'] else 'PARTIAL - Some fallbacks detected'}",
        "",
        "Evidence Sources by Category:"
    ])
    
    for cat_name, source_type in verification['evidence_sources'].items():
        if source_type == 'enhanced_backend_extracted':
            status_emoji = "üöÄ"
        elif source_type == 'backend_extracted':
            status_emoji = "‚úÖ"
        else:
            status_emoji = "‚ö†Ô∏è"
        report_lines.append(f"{status_emoji} {cat_name}: {source_type}")
    
    final_report = "\n".join(report_lines)
    logger.info(f"‚úÖ Generated comprehensive report with {len(all_categories)} categories")
    logger.info(f"üîç Data verification: {verification['backend_data_count']} real, {verification['fallback_count']} fallback")
    
    return final_report

def analyze_cv_file(cv_file_path: str) -> str:
    """Analyze a CV file and generate enhanced TXT report"""
    if not os.path.exists(cv_file_path):
        raise FileNotFoundError(f"CV file not found: {cv_file_path}")
    
    # Read CV file content
    try:
        if cv_file_path.endswith('.txt'):
            with open(cv_file_path, 'r', encoding='utf-8') as f:
                cv_content = f.read()
        else:
            # For PDF/DOCX, would need to extract text first
            raise ValueError(f"File type not supported for direct reading: {cv_file_path}")
                
        logger.info(f"üìÑ Loaded CV content: {len(cv_content)} characters from {cv_file_path}")
        return generate_comprehensive_enhanced_txt_report(cv_content)
        
    except Exception as e:
        logger.error(f"‚ùå Failed to process CV file: {e}")
        raise

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("‚ùå Usage: python enhanced_txt_generator.py <cv_file_path>")
        print("  Example: python enhanced_txt_generator.py sample_resume.txt")
        sys.exit(1)
    
    cv_file_path = sys.argv[1]
    
    try:
        # Generate the enhanced report from real CV file
        enhanced_report = analyze_cv_file(cv_file_path)
        
        # Save to file
        output_file = "comprehensive_enhanced_resume_analysis.txt"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(enhanced_report)
        
        print(f"‚úÖ Comprehensive enhanced TXT report generated: {output_file}")
        print(f"üìä Report length: {len(enhanced_report)} characters")
        print(f"üìÑ Source CV: {cv_file_path}")
        
    except Exception as e:
        logger.error(f"‚ùå Report generation failed: {e}")
        print(f"Error: {e}")
        sys.exit(1)